1. Q. ETL 파이프라인이란 무엇인가?
	1. 데이터를 수집, 처리, 적재하는 자동화된 워크플로우이다.
	2. 수집(Extract)
		1. **데이터 소스**로부터 데이터를 추출
	3. 처리(Transform)
		1. 추출한 데이터를 **비즈니스 규칙에 맞게 변환**
		2. 데이터 클렌징, 통합, 변환
	4. 적재(Load)
		1. 데이터를 **목적 시스템**(DW, DM 등)에 저장
2. Extract 단계의 목적과 기능
	1. 데이터 소스 식별 및 연결
	2. 성능 최적화
	3. 실패 복구 및 재처리 전략
		1. 읽다가 만 *빅데이터를 지탱하는 기술*에 복구 케이스가 소개되어 있다.
	4. 대용량 데이터, 분산/병렬
		1. 분산 환경에서의 파티셔닝 대응
		2. 병렬 처리 제어
		3. 메모리 및 네트워크 병목 성능 이슈 대응
2. Transform 단계의 목적과 기능
	1. Data Cleansing
	2. Standardization
	3. 비즈니스 규칙 적용
		1. 새로운 컬럼을 생성하거나 값을 변경하는 단계
		2. 데이터 결합: 반정규화
	4. 품질 및 무결성 검증
	5. 성능 최적화(Scale-out)
		1. 분산 컴퓨팅 프레임워크 활용
		2. 분산 셔플의 비용? -> Partitioning, Broadcast Join(최적화)?
	6. 메모리 및 디스크 I/O
		1. 메시지 큐 및 버퍼
		2. 파티셔닝
	7. 스트리밍 VS 배치
		1. 스트리밍은 특히 중복, 장애처리를 위한 체크포인트와 상태 중요
	8. 오류 처리 및 재처리
	9. 품질 모니터링 및 알림
	10. **중요**
		1. 비즈니스 요구사항 만족
		2. 파티셔닝 및 클러스터링
			1. 중간 결과 저장 및 재사용
			2. 고가용성
		3. 버전 관리
3. Load 단계의 목적과 기능
	1. 목적 시스템에 맞춰 데이터 적재
	2. 무결성 및 품질 보장
		1. 데이터 프로덕트에 따라 전략이 다르다.
	3. 성능 및 확장성
		1. Bulk Load, 압축, 병렬 처리 등
		2. 적재 후에도 쿼리 성능 보장
	4. 장애 처리 및 재처리
	5. 병렬 동시성
		1. DB Lock
	6. 분산 파일 시스템 고려
	7. 증분 적재 전략

---
1. 은태님의 설명
	1. 크롤링(E)
		1. Extract 과정은 일종의 Transform 기능을 포함한다.
		2. 정제, 포맷 등
		3. 파일 시스템에 JSON 형식으로 저장한다.
		4. BeautifulSoup은 정적 웹 페이지를 처리하는데 특화된 라이브러리
	2. Transform
		1. JSON -> Table로 변환
		2. 요구사항에 따라 변환
			1. USD 기준
			2. 소수점 기준
	3. Load
		1. 이번 과제에서는 Pandas.DataFrame를 이용해 Memory에 적재한다.
		2. Load 이후 Pandas를 이용해 분석 - 출력을 처리한다.
	4. 프로젝트 구조
		1. 문제의 요구사항과 산출물이 명확히 제시되어있다.
		2. 요구사항에 따른 프로젝트 구조를 미리 정의하는 것이 좋겠다.
			1. QA: 작업 중 실수와 혼선 방지
2. bs4를 이용한 크롤링 과정
	Q. 특정 태그를 기준으로 데이터를 추출하고 싶은데 일반적으로 어떤 과정으로 수행하나?
	1. 페이지 구조 분석
		1. 페이지 소스 및 개발자 도구 활용
	2. 태그 선택
	3. 크롤링 스크립트 작성
	4. 