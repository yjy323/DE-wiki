## 리뷰
### 강의
- Spark Job 최적화
- 오늘 강의 내용을 다시 학습하고, Transform 코드에 꼭 녹여내자. 단계적으로 구현하고 최적화 하는 과정을 꼭 기록하자.
### 프로토타이핑
- 끝단의 모습은 결정되지 않았지만, 일단은 페이스 리프트 전후의 고객 반응을 수집하는 것으로 시작하자.
- 지금 단계에서는 전후 고객 반응이 어떻게 변화했고, 변화가 어떤 의미를 갖는 지 읽어내는 것을 목표로 한다.
	- 전체적으로 긍정적인지, 긍/부정 포인트는 어디이고 얼만큼 개선되거나 후퇴했는지 수치로 나타내보자.
	- 여러 데이터에서 Indicator를 뽑아낼 수 있을지 고민하자
### 프로젝트
1. 각자 커뮤니티 하나씩 맡아 크롤링 코드를 작성하자.
2. 더 좋은 방식으로 파이프라인을 구축할 수 있도록 각자 AWS 서비스 하나씩 맡아 Internal한 동작까지 학습하자. 나는 Redshift를 중심으로 Data Lake(S3)에서 Data Warehouse(Redshift)로 ETL하는 부분을 담당한다.
## 회고
### KEEP
- 주말+오늘까지 하기로 한 작업을 어떻게든 일정에 맞춰서 완수했다.
	- AWS 서비스를 이용한 파이프라인 구성 및 테스트
	- Data Ingestion을 위한 크롤링 코드도 개발 완료했다.
- 아직까지는 일정 관리와 병렬적인 작업이 잘 이루어지고 있다.
	- 특히 코드/프로토타이핑 어느쪽도 크게 발목 잡지 않고 있다.
### PROBLEM
- 구글 검색을 직접하지 않고 requests를 이용해 파라미터만 바꿔가면서 크롤링을 시도했다. 단 한번의 시도, 1초만에 블락 당했다. 우회 방법을 미리미리 숙지해야겠다.
### TRY
- 이제는 본격적으로 코드 차원에서 협업해야 하므로 데일리 스크럼 때 코드 레벨로 공유를 해야겠다.
	- Git도 일정한 규칙에 따라서 사용하자.
		- 우선 Github project와 Github flow 및 커밋 컨벤션을 도입한다.
- 매일 아침 데일리 스크럼 - 아이디어 조금씩 구체화 - 유즈 케이스 도출 - 아키텍처 디벨롭 및 일정 구체화 단계로 협업하자, 내일 의견을 내봐야겠다.