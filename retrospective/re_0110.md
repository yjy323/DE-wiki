## 리뷰
### 데일리 스크럼
- 오늘 할 일:
	- 코드 리뷰
	- 주영: API를 이용하는 Extract 프로세스 추가, 코드 리뷰 및 다노님 피드백 반영한 리팩토링
	- 은태님: 코드 리뷰 중 궁금했던 코드의 deep한 동작 이해
		1. pandas.DataFrame.*to_sql*
	- 도한님: API를 이용하는 Extract 프로세스 추가, 코드 리뷰 및 다노님 피드백 반영한 리팩토링
### 코드 리뷰
- 함께 과제를 이해하고 커뮤니케이션 하는 과정에서 전체적으로 비슷한 구조로 구현하였다.
	- Extract - Transform - Load를 나누는 기준은 논리적인 단위와 함께 물리적인 제약사항과 장애 대응 및 복구 용이성으로 한다.
	- Extract 단계는 raw HTML에서 GDP 테이블 데이터를 추출하고 JSON을 Staging하게 저장하는 책임을 갖는다.
		- 도한님은 raw HTML을 DB에 담는, 일종의 ELT 프로세스도 함께 구축을 해보았다.
	- Transform의 책임은 비슷하지만 구현 방법이 달랐다.
		- 주영과 도한님은 pandas.DataFrame을 이용해 column 단위로 transform을 처리했다.
		- 은태님은 dictionary와 apply 등을 이용해 직접 데이터를 조작하였다.
			- 데이터 변환 과정 중 로깅과 장애 대응을 고려한 코드
	- Load의 책임은 비슷하지만 구현 방법이 달랐다.
		- 주영과 도한님은 pandas.DataFrame.to_sql을 활용해 비교적 간단하게 처리하였다.
		- 은태님은 데이터 row마다 insert문을 직접 호출해 처리하였다. 마찬가지로 데이터 변환 과정 중 로깅과 장애 대응을 고려한 코드
	- 예외처리 및 로깅에 대한 상이한 접근
		- 협업 시에는 예외처리와 로깅 전략을 통일해야 할 것이다.
		- Q. 언제는 전체 프로세스가 중단되도록 예외를 터트리고, 언제는 오류 검사를 통해 일부 프로세스는 중단하고, 일부는 마저 실행하도록 해야할까?
		- A. 웹 스크래핑을 통해 다양한 형식의 데이터를 추출하는 Extract 과정이라고 하자. 하나의 큰 ETL 프로세스에서 이미지, 리뷰 데이터를 각각 추출하는 과정인데, 이미지 데이터를 추출하는 도중 예외가 발생했다면 에러를 핸들링하고 리뷰 데이터는 마저 추출할 수 있다.
		  전체 프로세스를 재실행하는 것은 비효율적이다. 이미지 데이터는 별도의 장애 대응 방법에 따라 처리하면 될 것이다.
- Data Product VS Prototype
	- Prototype을 만들 때는 pd.to_sql() 등을 사용해 간략하게 데모를 만들 수 있다.
	- 반면 Data Product를 만들 때는 체계적으로 로깅이나 예외처리를 해야한다. 따라서 to_sql() 대신 직접 정의한 최적화된 코드를 작성해야 하겠다.
- 은태님:
	- 위키피디아 URL, JSON 저장 위치와 같은 값을 ENV로 관리할 수 있겠다.
	- 파이썬에서 Machine 독립적으로 디렉토리 경로를 지정할 수 있는 Path String의 존재
	- Transform 또는 Load 과정 처리 중간에 에러가 발생할 경우 해당 지점을 로그 등에 기록할 수 있도록 로직을 파이썬으로 직접 구현
		- Q. 에러가 발생한 지점부터 프로세스를 재실행하거나 복구할 수 있는 방법은 무엇일까?
		- A. 해당 지점을 별도의 모니터링 파일 등에 저장하고 CI/CD 워크플로우에 통합할 수 있을 것이다.
			- 재실행 프로세스를 독립적으로 실행할 수 있도록 모듈 단위를 더 잘게 나누고 파라미터를 수정해야 할 것 같다.
- 도한님:
	- API 호출을 병렬로 처리하기 위해 멀티 프로세싱을 도입하는 중이다.
		- API 호출은 소켓 I/O의 오버헤드가 크기 때문에 멀티 프로세싱 대신 멀티 스레딩을 도입하는게 좋을 것 같다.
		- 디버그 도구를 통해 프로세스 또는 스레드 할당 및 사용 현황을 볼 수 있다?
			- 미리 작업 계획을 세울 수 있을 것이다.
- 주영:
	- 예외처리 및 로깅
		- Fetch, File, Memory 에러 등은 예외를 raise VS 파싱 에러는 None을 리턴
			- 의도와 목적이 명확한가? 그렇지 않은 것 같다.
			- 기준을 갖고 프로세스를 중단해야할 예외와 그렇지 않은 예외를 분리해야 할 것이다.
		- 예외 로그가 너무 길어질 것 같아서 Exception 정보를 기록하지 않았다. 하지만 대화를 해보니 별도의 파일 또는 그냥 로그에 기록을 남겨두는 것이 좋겠다.
	- DB I/O VS ConnectionPool
		- 주영은 Load 작업 중 매 쿼리마다 Connection을 새로 얻고 반환하였다.
		- 은태님은 전체 로드 작업에서 Connection을 한번 얻고 완료할 때 반환하였다.
		- Q. 새로운 커넥션을 자주 할당받으며 생길 오버헤드를 고려했을 때, 한번 커넥션을 얻고 파라미터로 전달하도록 리팩토링 하는 것이 어떤가?
			- A. 이번 코드에서는 적절한 지적이다. 큰 고민을 하지 않고 작성한 것 같다.
		- Q2. 그렇지만 Connection을 한번만 얻고 모든 쿼리를 실행할 때 생기는 문제는 없을까?
			- 대용량 데이터를 Load하기 위해 멀티 스레드를 적용했는데 Connection이 말라버리면 의도대로 동작하지 않을 것이다.
			- 프로세스의 목적에 따라 구현 방식이 다양하겠다는 것을 다시 한번 느꼈다. -> **생각의 중요성**
	- API Extract를 어떻게 구현할까?
		- 웹 스크래핑 후 과제를 구현할 때는 HTML \<tr\> \<td\>대로 JSON에 자연스럽게 저장할 수 있었다.
		- 우리 팀은 데이터의 갱신으로 인한 과거 데이터는 timestamp를 도입하고 JSON은 중첩없이 직렬화 하는 방법의 장점에 공감했다.
			- 예를 들어 {'국가': KOR, 'GDP': 9,000, 'YEAR': 2024}, {'국가': KOR, 'GDP': 10,000, 'YEAR': 2025} 와 같은 형식이다.
			- 반면  {'국가': KOR, '2024': 9,000, '2025': 10,000}으로 저장할 수도 있다.
				- 이를 테이블로 표현하면 일종의 "국가 - 연도"를 기준으로 하는 Pivot Table과 같을 것이다.
		- Q1. API 호출 결과는 {'국가': KOR, '2024': 9,000, '2025': 10,000}와 같은 형태인데, 이를 어떻게 직렬화할 것인가?
			- Extract 이후 Transform 과정에서 처리해야 할 것이다.
		- Q2. ETL 프로세스 실행 시 **전체 기간**에 대한 데이터를 추출할 것인가?
			- 그렇다면 이후 새로운 데이터가 추가되었을 때, 중복되는 과거 데이터를 어떻게 처리할 것인가?
			- A1. Extract 이후 Transform 과정에서 Cleansing 할 수 있다.
			- A2. Extract 시 전체 기간에 대한 데이터를 추출하지 않고 매년 새로 데이터를 추출해 append 할 수 있다.
				- 연도를 파라미터로 API를 여러번 호출 하는 것
				- 특정 국가의 GDP를 수집하지 못했고, 따라서 API 결과를 응답받을 수 없다면 어떻게 핸들링할 것인가?
### 코드 리팩토링
1. API Extract 코드 구현 과정
	1. API 요청 - 응답 시간은 1s - 3s로 지연이 꽤 크다.
	2. 데이터는 일정 기간마다 연도 기준으로 append 될 것이다.
	3. append에 유리한 JSONL 포맷을 사용한다.
	4. append에 용이하도록 응답의 메타 데이터를 제외한다.
2. API Transform 코드 구현 과정
	1. Extract 과정에서 append 시 중복 데이터가 저장될 수 있다.
	2. Transform 과정에서 cleansing 한다.
**[구현하지 못한 아이디어]**
1. GDP 외에 다양한 자료에 대한 API 요청을 지원한다.
2. Transform
	1. GDP 국가 코드와 국가 명을 JOIN 한다.
	2. GDP 국가 코드와 ISO-3166으로 Region을 JOIN 한다.
	3. 비즈니스 룰에 따라 데이터를 조작한다.
	4. Extract 과정에서는 아래 형식으로 데이터를 저장한다.
		{'KOR': {'2025': 10,000}}
		{'KOR': {'2024': 9,000}}
		이를 아래와 같이 변환해 테이블 기준 컬럼 명을 일치시킨다.
		{'country': 'KOR', 'gdp': 10,000, 'year': 2025}
		{'country': 'KOR', 'gdp': 9,000, 'year': 2024}
3. 대용량 데이터, 병렬/분산 대응
	1. pandas.DataFrame의 메서드를 이용해 transform하면 메모리가 부족할 수 있다.
		1. chunk 단위로 데이터를 변환하고 문제가 생긴 지점을 기록한다.
	2. pandas.DataFrame.to_sql을 사용한 로드 작업을 어떻게 개선할 수 있을까?
		1. 팀원들과 함께 to_sql()의 시그니처를 분석
		2. 우리의 목표는 to_sql()을 사용하면서 중간에 장애가 발생한 지점을 기록할 수 있는지, 복구할 수 있는지 방법을 찾는 것이다.
		3. to_sql()은 insert만을 위한 용도가 아니다. 기본 동작이 insert로 정의되어 있을 뿐, method로 전달한 callable object를 실행할 수 있다.
			1. callable object에 로그를 기록하고 장애에 대응할 수 있는 사용자 정의 메서드를 전달해 해결 할 수 있다.
			2. 그렇다면 to_sql()을 사용하지 않고 사용자 정의 메서드를 사용하면 될 것 같다.
		4. 목표로 하는 기능이 명확하다면 사용할 수 있는 방법이 무엇인지, 정확한지 검토할 수 있다.
## 회고
### Keep
- 팀원끼리 생각은 많이 공유하고 소통했지만 코드 레벨의 자세한 리뷰는 오늘이 처음이었다. 오전 시간을 모두 할애해 깊이 있는 기술적인 대화를 했다고 생각한다. 어떻게 코드를 작성했는지 단순히 공유하는 것보다 어떤 관점에서 어떻게 개선할 수 있을 지 서로 고민하고 질문을 한 시간이었다.
- 로직의 개선에 대한 지엽적인 리뷰보다는 앞으로 과제나 프로젝트를 하면서 어떻게 개선점을 적용할 수 있을 지 서로 리뷰를 한 시간이었다.
- 생각을 어떻게 해야할까?에 대한 다노님의 답은 "생각을 할 수 있는 cue를 찾으세요" 였고 방향성을 잡을 수 있었다.
	- 오늘만 해도 DB 커넥션 관련 코드 리뷰 중 비슷한 경험을 했다.
		- 별다른 생각 없이 DB 커넥션을 매 쿼리마다 할당받고 반환했다. 그렇지만 은태님의 피드백은 한번 할당 받고 전체 쿼리를 실행하는 것이 좋겠다는 것이었다.
		- 이미 새로운 아이디어를 얻었지만 여기서 한번 더 생각해보니 '반대로 커넥션을 매 쿼리마다 실행하는 것의 장점은 없을까?'라는 질문을 던질 수 있었다.
		- 실행 시간이 굉장히 오래 걸리는 쿼리가 존재하고 멀티 스레딩 환경이라면 한 스레드가 커넥션을 오래 들고 있는 것이 비효율적일 것이라는 아이디어가 떠올랐다.
		- 이번에도 목적이나 환경에 따라 구현 방식이 다를 것이라는 결론을 내렸고, 그 결론에 다다르도록 생각하는 법에 대해 나름의 경험을 쌓게 되었다.
	- 질문을 잘해야겠구나 싶다.
### Problem
- 감기 기운이 갑자기 심해지면서 목표한 만큼 학습하지 못했다. 주말 동안 컨디션을 끌어 올려 다음주에는 지장이 없도록 해야겠다.
### ​Try
- 일주일동안 그룹 별로 학습 방식이 조금씩은 달랐던 것 같다. 각자에게 익숙한 방식이 있을텐데 다음주에는 어떻게 진행할 지 소통을 잘 해봐야겠다.
- 팀원들끼리는 이미 일주일동안 시간을 오래 보냈으니, 다음주에는 그룹 별로 더 자연스럽게 섞여서 질문하고 피드백을 받을 것이다. 은태님은 클라우드 경험이 많은데, 다음주에는 어떤 아이디어로 문제에 접근할 지 궁금하다.