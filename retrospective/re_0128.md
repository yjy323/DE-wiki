## 리뷰
### 프로젝트
IP-R&D 아이디어는 아직 프로토타이핑을 하지 않아서 구체성이 부족하다. 프로토타이핑을 더 하고 피드백 받으면서 *생각하는 방법을 더 기르고 싶지만, 지금은 다음 단계로 넘어가 또다른 부분에서 생각하는 법을 배워야 할 것* 같다. 팀원들과 함께 브랜드 창업 아이디어의 fidelity를 높이기 위해 근거를 조사하고 프로토타입을 좀 더 구체화하고 데이터 수집을 위한 개발에 착수하는 과정이 필요하다.
#### 회의
- 개발 단계 계획을 세우려고 했는데, 아직은 프로토타입에 대한 근거가 잘 마련되지 않은 것 같다. **우선은 자료조사를 진행**하면서 수집해야 할 **데이터의 우선 순위를 결정**하고 **프로토타입을 더 구체화**하고 피드백 받으면서 방향성을 예리하게 잡아야 할 것 같다.
#### 자료조사
- 초기 아이디어 명 "최근 트렌드인 브랜드 수명 분석" -> "프랜차이즈 창업 추천도 평가" ? 팀원과 의논
- 상권 분석 또는 트렌드 분석 논문에서는 문제를 어떻게 접근하는가?
	- 어떤 알고리즘을 적용했고, 어떻게 모델을 학습시켰는지는 아직 중요하지 않다.
	- 연구자들은 어떤 데이터를 왜 수집해야겠다고 생각했을까?
	- 창업 추천도를 잘 평가하는 일은 연구에 가깝지 않나? 라는 생각이 들었지만 우리는 모델의 구조에는 큰 관심이 없고 선행 연구에서 유의미하다고 해석한 지표를 다방면으로 수집·가공하기 때문에 Product에 가깝다고 말할 수 있겠다.
	
**수집할 데이터 소스를 결정하기 전 "생각"의 과정**
1. 비즈니스 목표
	- 특정 상권에 급격하게 트렌드를 따르는 프랜차이즈를 신규 창업해도 좋을지 의사결정하기 위한 Data를 제공하는 Product
2. **핵심 질문** 던지기
	- 이 프랜차이즈의 트렌드는 얼마나 오래 유효할까?
	- 이 프랜차이즈의 아이템은 이 상권에 얼마나 fit한가?
	- 이 상권은 트렌드에 얼마나 민감한가?
	- 이 프랜차이즈로 창업했을 때 투자 대비 얼마나 큰 이익을 볼 수 있을까?
	- ... 지금은 다양한 핵심 질문이 중요한 것 같다.
3. 도메인 리서치 및 문헌 연구
	- 상권 분석 관련 개념 이해
	- 사례 분석 및 경쟁 제품 조사
	- 논문 및 보고서, 통계 연구
4. 데이터 범주 설정
	1. 인구 통계 지표
	2. 경제 지표
	3. 경쟁도, 접근성, 부동산 등 상권 지표
	4. 브랜드 평판 및 트렌드, 성장 가능성
	5. ...
5. 구체적인 데이터 결정

- Q1. 트렌드에 특히 집중을 할 것인가? 수치 데이터를 포함해 광범위한 데이터 수집이 필요한가? 나는 후자라고 본다. 전자는 연구에 가깝지 않나?
- Q2. 지금 단계에서 우리 프로덕트의 KPI가 무엇인지 고민해야할까?
### Spark
오픈소스 분산 데이터 처리 프레임워크로 Hadoop *MapReduce의 한계를 극복*하고자 개발

**특징**
1. 빠른 데이터 처리 속도
	1. In-memory Processing
	2. 하드웨어 리소스 비용이 크다.
2. 유연한 프로그래밍(High-Level)
3. 다양한 작업 지원/다목적 플랫폼
	1. Batch
	2. Streaming
	3. 대화형 쿼리
	4. DS/ML
4. Spark는 JVM 위에서 동작한다.

**Driver**
- Spark application의 메인 프로세스로 크게 SparkContext와 Schedular로 구성된다.
- Job을 계획(DAG 생성), 분배(Task 생성 및 executor 실행 요청)하고 모니터링 및 결과를 수집한다.
- Driver는 각 Executor와 직접 communication한다.
	- RPC 프로토콜을 이용한다.

**Executor**
- 실제 데이터 처리와 연산을 수행한다.
- driver에게 task를 할당받고 상태를 주기적으로 report 한다.
- 모든 executor는 독립적으로 동작한다.

**Cluster Manager**
- Spark 클러스터 노드들의 리소스를 관리하고 Driver와 Executor 사이의 작업 분배를 실제로 조율한다.
- YARN Cluster 모드에서 Spark Driver는 ApplicaionMaster 컨테이너에서 동작한다.
	- 기존 AM의 역할은 RM과 직접 통신하며 리소스를 할당받고 애플리케이션 상태를 report하는 역할로 축소된다.
	- 컨테이너(Executor) 실행에 필요한 **리소스 결정과 Executor와의 통신**은 Driver가 담당한다.
- 작업 단계 in YARN
	1. Job 제출
	2. ApplicationManager의 Job 등록, Schedular에 보고
	3. ApplicationMaster를 생성할 NodeManager 결정
	4. AM 및 Spark Driver 실행
	5. Spark Driver의 작업 계획 및 실행할 Executor와 필요한 리소스 결정
	6. AM이 RM에 컨테이너 할당 및 실행 요청
	7. 컨테이너(Executor)를 실행할 NodeManager 결정
	8. Executor 실행 및 연산 작업 처리
		1. 작업 과정에서 YARN과 독립적으로 Driver와 통신
	9. 작업 완료 보고 및 결과 수집, 저장
- Data Locality
	- Spark Driver가 HDFS 블록의 위치를 보고 데이터와 가까운 노드에 **Executor를 요청하는 주체**
	- RM은 노드들의 상태를 확인하고 최대한 **가까운 노드에 Executor를 할당하려고 노력**한다.

**Deploy Options**
- *어떤 클러스터 매니저*를 사용할 것인가?
	- Standalone
	- YARN
	- Mesos
	- Kubernetes
- 배포 목적에 따라 선택한다.
	
**Deploy Modes**
- *Driver를 어디서 실행*할 것인가?
	- Local
	- Client
	- Cluster
- 운영 목적에 따라 선택한다. 테스트와 디버깅에는 Client Mode, 안정성 및 대규모 처리는 Cluster Mode를 선택한다.

**RPC 프로토콜**
- Remote Procedure Call
- 타 머신에 위치하는 함수를 로컬 함수처럼 사용하도록 원격 노드에 작업을 요청하고 응답 받는 네트워크 기반 함수 호출
- Spark의 Driver와 클러스터 노드들이 서로 다른 머신 또는 컨테이너에 있을 때 RPC를 이용해 통신한다. 함수의 호출에 네트워크 자원을 사용하므로 Driver와 노드가 **물리적으로 최대한 가깝게 위치**하는 것이 좋다.
- **Q. Spark에서 RPC를 사용하는 이유는?**
	- 여타 네트워크 프로토콜보다 통신 오버헤드가 작다.
	- 빈번한 소규모 요청/응답을 빠르게 처리하도록 설계되어 분산 환경에서의 통신에 유리하다.
	- 비동기 통신을 지원한다.
- **Q. 단점은?**
	- Latency에 민감하다. Driver와 노드가 물리적으로 멀리 떨어져 있으면 Spark 작업 성능이 저하된다.
	- 대용량 데이터 전송에는 한계가 있다.
	- 보안성이 좋지 않다.
#### W4M1 & Standalone Cluster
1. Spark 설치 및 Configuration
2. Spark Job 실행 및 결과 확인
3. Spark 클러스터 설정과 Job 실행 과정에서의 에러 처리와 디버깅을 고려한다.
4. Dockerfile과 docker-compose를 이용해 1, 2, 3의 과정을 쉽게 재현한다.
	1. Interactive하게 Dockerfile을 작성하자
5. 위 과정을 통해 Spark 아키텍처가 실제 Cluster로 어떻게 구축되는지 이해하고 원리와 예상 가능한 이슈는 무엇일지 꼭 생각해본다.

**[Apache Spark standalone architecture](https://medium.com/yavar/spark-standalone-architecture-dafe8fe0d2e)**
- YARN 같은 별도의 클러스터 매니저 없이 동작하는 클러스터
- 클러스터의 구축과 실행 과정을 쉽게 학습, 테스트, 디버깅하기 좋다.
- 클러스터를 구축하면서 노드들이 어떻게 할당, 생성되고 서로 통신하는지 이해한다.
- Job을 제출하면서 일어나는 과정을 이해한다.
	- Q. Fault Tolerance는 어떻게 보장되지?
## 회고
### KEEP
- W3에서는 이론 공부를 거의 하지 않고 바로 미션부터 붙들면서 여러 이슈에 제대로 대응하지도, 충분히 학습하지도 못했다. YARN을 이미 배워서 이해가 빠른 것도 있지만, 이번에는 미션의 요구사항을 보고 내가 실제로 어떻게 해야할지 명확히 구상할 수 있었다. 이대로 빠르게 W4M1을 구현하며 실습하고 다시 돌아와 더 깊고 넓게 학습해보자.
### PROBLEM
- 아이디어의 현실성은 어느 단계에서 평가해야할까? 지금은 프로토타이핑 이후 근거를 마련하고 fidelity를 높이는 과정에서 평가해야할 것 같다고 느껴진다.
  과거에 여타 프로젝트를 했던 경험을 떠올려보면 제대로 프로토타이핑 하기도 전에 실현 가능성을 많이 따졌던 것 같다. 프랜차이즈 창업 추천 리포트가 실현 가능할까? 라는 생각이 자주 들지만 그것도 데이터에 매몰되기 때문인 것 같다. 지금은 단계별로 생각하는 힘을 길러야 하겠다.
### TRY
- 우리가 제시할 Actionable Data의 근거를 우선 마련하고 연관성이 있어보이는 데이터를 다방면으로 수집하는 단계로 개발하자. 데이터를 무작정 수집하고 거기서 관계성을 뽑아내려고 하면 아이디어를 구체화할 때 빠진 함정에 똑같이 빠지게 될 것 같다.
- 비즈니스 목표 설정 > **핵심 질문 던지기** > 리서치 및 연구 > 데이터 범주 설정 > 구체적인 데이터 소스 결정 순으로 진행해봐야겠다.
	- 프로토타이핑을 통해 일차적인 비즈니스 목표는 설정했다. 그저 많은 데이터를 뜯어보고 쓸 지 말지 고민하는게 아니고, 비즈니스 목표 달성을 위한 핵심 질문을 잘 던지고 답변을 상상하면서 범위를 좁히고 연구를 통해 결정해야 한다.
		1. "커뮤니티 검색량과 트렌드는 어떤 상관 관계를 가지나요?" in 다노님 피드백
		2. A 프랜차이즈의 트렌드는 얼마나 오래 유효할까?
		3. A 프랜차이즈의 아이템은 이 상권에 얼마나 fit한가?
		4. 이 상권은 트렌드에 얼마나 민감한가?
		5. A 프랜차이즈로 창업했을 때 투자 대비 얼마나 큰 이익을 볼 수 있을까?
		6. ...
	- 이렇게 놓고 보니 개발 전 단계에 쏟아야 할 리소스가 정말 크다. 5주차 프로젝트에서는 이 시간을 정말 잘 관리해야겠다.
- W4M1을 구현하면서 다음 질문과 생각을 꾸준히 던져보자.
	- Spark 아키텍처 구조, 실행 과정 그림과 연결 짓기
	- 예상 가능한 이슈는 무엇이고 HA와 Fault Tolerance를 어떻게 보장할 수 있을까?
		- Spark와 Hadoop의 특징, 각 구성요소들의 책임과 연관 짓기