## 리뷰
### 미션
#### W4M1
- 예제 스크립트를 작성한다.
- 도커 볼륨을 지정해 스크립트와 입출력 데이터, 로그를 영속화한다.
- W4M1의 스크립트는 어떤 과정으로 처리 됐을지 분석해봐야 한다.
#### W4M2
- 로컬 머신에 spark를 직접 설치 및 설정하고 pyspark를 이용해 Local 모드로 실행한다.
- 따라서 분산 처리의 이점은 경험하지 못했다.
- Spark Pandas API를 활용해본다는 의의가 있었다.
- 코드를 작성하는 과정에서는 효율적인 코드인가? 분산 환경에서는 의도대로 동작할까? 라는 질문을 던지고 답을 하려고 노력했다.
- 하지만 API가 어떻게 구현되었고, Spark 상에서 어떻게 처리되는지 아직은 잘 모르기 때문에 충분하지 않았다.
- 아주 대용량 데이터가 아니었는데도 메모리 이슈로 Spark 커넥션이 종료되는 일이 꽤 잦았다. 또, Spark 실행 환경 설정은 간단하지 않고 환경에 민감하다고 느꼈다. 이 과정에서 Docker의 중요성을 느낄 수 있었다.
- 따라서 추후 Docker를 이용해 주피터를 통해 접근할 수 있는 분산 클러스터 환경을 구축할 계획이다.
### 프로젝트
- **핵심 질문**을 던지며 프로토타입을 더 구체화하고 개발하자는 의견을 나눴었는데, 비대면 상황에서는 어려움이 있었고 작업 일정이 너무 지체된다고 느껴졌다. 따라서 그 과정을 최소화해 '트렌드를 파악하기 위한 커뮤니티 데이터', '매출 예측을 위한 데이터', '상권 분석을 위한 부동산 데이터'를 선정하였다.
- 각 데이터를 수집하고 처리하면서 또 다른 방식의 생각을 배우고 프로젝트 일정을 관리하는 법을 배울 수 있을 것이다.
- 특히 이전까지 일정 관리에 큰 어려움을 겪었기 때문에 협업 환경을 잘 구축해야 한다.
- 미션은 보통 혼자 해결하다보니 Git을 제대로 활용하지 않고 PR 제출에만 썼는데, 프로젝트 중에는 Github flow 전략과 Github project를 이용해 구체적인 일정과 히스토리를 관리할 계획이다.
## 회고
### KEEP
- Pandas API는 이미 이해도가 있고, Spark도 어느정도 학습하고 미션을 풀어보니 이슈가 발생해도 예외를 해석하고 해결 방법을 더 빠르게 찾을 수 있었다.
	- Parquet 스키마 관련 에러는 명확하게 해결하지 못했다. Parquet도 Spark도 아직 잘 모르는 부분이 많기 때문이다.
### PROBLEM
- 일정 관리에 굉장히 어려움을 느끼고 있다. 그만큼 생각하고 팀원들과 의견을 나누는데는 시간을 많이 들였지만, 이제는 일정과 히스토리를 타이트하게 관리하는 연습도 필요한 순간이라고 느낀다. Github flow 전략과 Github project를 활용해보자.
### TRY
- 매일 Spark API를 조금씩 학습하며 인터페이스가 내부적으로는 어떻게 구현되었고 어떤 동작을 하는 지 위키에 차곡차곡 정리하자. 더 효율적이고 최적화된 코드를 작성하고 예상 가능한 이슈를 생각하는 습관에 도움이 될 것이다.