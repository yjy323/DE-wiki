## 리뷰
### 데일리 스크럼
- **어제 한 일: 어제 서로 고민한 내용들을 나누고 정리함**
  - ETL 프로세스는 처음 배우고 구현해보았다.
  - 나는 [**데이터 소스 - (ETL) - DW - (ETL) - DM 구조**]로 이해했다. 따라서 파일 시스템은 DW에 대응되는 개념이라고 이해했다.
  - 그러나 주어진 과제는 [**데이터 소스 - E - 파일 시스템 - TL - DB**] 라고도 표현할 수 있다.
  - ETL 프로세스 사용 목적과 제약 사항에 따라 구조가 달라질 것 같다.
    - 목적이 아닌 과제와 다노님의 의도에 맞추려 하니 어려웠다고 생각한다.
    - 분석 관점에서는 어떤 목적을 위한 ETL인지 공유해야 하겠다.
    - 병렬/분산 처리에 용이하고 대용량 데이터를 처리하기에 좋은 구조인가? 라는 질문에 대한 답은 프로세스의 독립적인 단계를 나누는 기준이 되겠다.
  - ETL 프로세스의 아키텍처의 각 요소는 어떤 역할과 책임이 있나?
    - 관점과 근거에 따라 기준이 다양하다는 것을 느낌
    - 아키텍처를 정의하고 설계하는 일은 완벽에 가까워야 할까?
      - 모든 문제에 완벽하게 대응 가능한 설계란 현실적으로 불가능하다.
      - 설계 과정에서 개인/팀의 비용이 많이 소모되는 것을 경험함, 대응 방안은 다음과 같다.
        1. 일단 당장 주어진 문제를 해결할 수 있는 구조로 느슨하게 설계하고 구현한다.
        2. 문제 발생이 예상되거나, 실제로 발생하는 지점을 개선한다.
    - 은태님과 도한님의 경험과 생각은 어떤지 들어보고 싶었음
      - 두분 다 비슷한 결론으로 동의함
  - 좋은 ETL 프로세스에 대한 고민이 필요하다.
    - pd.DataFrame의 장점이 뭔지 잘 알고 그것을 살리는 코드였나? 지금은 잘 모르겠다.
    - 외부의 변화에 대응할 수 있는 코드를 작성하고 싶었는데, 역량 부족으로 우선은 주어진 문제인 'IMF GDP 통계를 이용한 분석 수행'을 해결하는데 집중했다.
      - 다만 어떤 지점에서 외부의 변화에 대응할 수 있을 지 고민해본다.
        - Wiki Table의 1-3열 까지의 데이터를 일괄적으로 가져오는 대신 'Country/Territory'와 'IMF' 컬럼을 이용해 attribute를 가져오도록 개선한다.
- 오늘 할 일:
  - 팀 활동 요구사항 및 추가 요구사항을 해결한다.
  - 기존 코드를 개선한다.
    1. 주석
    2. 로그
    3. 대용량 및 분산/병렬 환경 고려
- 오늘 한 일:
  - 팀 활동 요구사항
    1. Q. wikipeida 페이지가 아닌, IMF 홈페이지에서 직접 데이터를 가져오는 방법은 없을까요? 어떻게 하면 될까요?
       - A. 질문을 바꾸어 범위를 좁힌다. "IMF 홈페이지에서 직접 데이터를 가져오는 방법"을 "웹을 데이터 소스로 사용하는 환경"으로 바꾸어본다.
       - 웹 환경에서 데이터를 Extract 할 수 있는 방법은 아래 2가지로 분류할 수 있고, 모두 ETL 프로세스로 자동화할 수 있다.
         1. API
         2. 웹 스크래핑
            1. HTML
            2. 웹 애플리케이션에서 제공하는 정적 파일
     - 따라서 IMF 홈페이지에 직접 웹 스크래핑을 적용하거나, IMF가 제공하는 API를 활용해 데이터를 가져올 수 있다.
    2. Q. 만약 데이터가 갱신되면 과거의 데이터는 어떻게 되어야 할까요?
     - *데이터의 갱신*은 같은 데이터가 갱신되는 경우, 예를 들어 2025년 IMF 데이터가 새로 측정되어 갱신되는 경우로 해석한다.
       - 데이터를 Extract 하거나, Transform 하거나, Load 한 시점 등을 timestamp로 저장한다.
     - 도한님은 디스크를 효율적으로 사용하기 위해 timestamp 컬럼을 추가하지 않고 disk의 offset 등을 활용하자는 의견을 제시했다.
       - Embbeded 같이 잘 통제할 수 있고 제한적인 환경이라면 실현 가능할 것 같다.
       - 대용량 데이터를 처리하기 위한 분산/병렬 환경에서는 실질적으로 적용하기 어려울 것 같다.
         1. offset 단위를 지정하기 어렵다.
         2. 분산 환경에 적용하기 어렵다.
         3. 컬럼을 추가해 얻는 이익이 디스크를 사용하는 비용보다 크다고 생각한다.
    3. Q. 과거의 데이터를 조회하는 게 필요하다면 ETL 프로세스를 어떻게 변경해야 할까요?
      - ETL 프로세스의 전체적인 구조는 동일하게 유지하되, 특정 시점에서 timestamp 컬럼을 추가할 수 있다.
      - timestamp를 어느 시점에 추가하는 것이 좋은가? 시점을 구분하는 프로세스 단위는 데이터소스 - DW - DM으로 간략히 표현한다.
        1. 데이터 소스에서 DW로 ETL 하는 시점
        2. DW에서 DM으로 ETL 하는 시점
        - 분석 목적과 아키텍처에 따라 다를 것 같다. 지식과 경험의 한계 때문인지 어떤 일관된 답을 도출할 수는 없었다.
     - timestamp를 이용해 version control system을 적용한다.
        - DW에서 DM로 ETL할 때, timestamp를 기준으로 특정 기간의 데이터만을 유용하게 사용할 수 있다.
      - QQ. ETL 프로세스 중 장애가 발생한다면 어떻게 대응해야 할까?
        - DB 관련 장애
        - 네트워크 관련 장애
        - 물리적인 장애
        - 구체적인 시나리오마다 다를 것 같다. 관련 경험과 지식이 부족해 대응 방안은 솔직히 잘 모르겠다. 학습 단계에서는 ETL 프로세스를 독립적으로 구성하는 것과 일어날 수 있는 상황과 그 원인을 아는 것이 우선이라고 생각한다.
  - 리팩토링:
    - Extract와 Transform 프로세스 코드를 리팩토링한다.
      - 최소한의 기능 단위로 모듈을 분리한다.
      - 모듈 별로 주석을 자세히, Pythonic하게 작성한다.
      - 기능 단위 별 실행 로그를 기록한다.
        - 리팩토링 과정에서 로그의 중요성을 느낄 수 있었다.
          - Q: 지금은 로그에 timestamp로만 구분되는데 같은 url로 동시에 요청이 전달된다면 어떻게 식별할 것인가?
          - 어떤 호스트는 응답에 성공하고, 다른 호스트는 실패했다면 실패한 원인을 어떻게 분석할 수 있을까?
          - 별도의 고유 식별자를 도입해 해결할 수 있다?
          - 로그 전략이 중요할 것 같다. 다양한 대용량 데이터가 복잡하게 처리되는 과정을 세세하게 로그로 기록해야 장애에 대응하고 최적화할 수 있겠다.
          - ETL 프로세스에서의 장애 대응에 대해 대화를 나누며 장애 원인을 파악하는 것이 가장 중요하다는 의견에 공감했었다. 장애 원인을 파악하기 위해서는 세세한 로그가 중요해보인다.
    - Load 프로세스는 sqlite3와 pd.to_sql()로 쉽게 구현할 수 있었다.
      - 하지만 분산/병렬처리에 대응 가능한가?
        - 데이터를 load하는 중간에 장애가 발생한다면 대응이 어렵다. 분산 환경에서는 특히 어려울 것이다.
        - load 과정 중간중간의 로깅이 어렵다.
      - 대용량 데이터에 대응 가능한가? 모든 데이터를 메모리에 올려 to_sql()을 호출할 수 없을 것 같다.(은태님 의견)

## 회고
### Keep
  1. 소통을 많이 하면서 좋은 소통 방법을 배우고 있다고 느낀다.
     1. 내가 무엇을 알고 무엇을 모르는지, 지금 궁금하거나 대화를 나누고 싶은 주제가 무엇인지 잘 설명하게 되는 것 같다.
     2. 다른 사람의 설명 또는 주장이 어떤 의미인지, 어떤 상황을 묘사하는 것인지 점점 빠르게 이해하게 된다.
        1. 문제, 장애 등 기술 관련 대화를 하다보면 서로 다른 상황이나 주제로 받아들이는 경우가 많은 것 같은데 그런 오해가 점점 줄어든다.
     3. 다양한 주제로 많은 대화를 나누는데 걸리는 시간이 줄어든다.
        1. 오늘은 ETL 프로세스와 대용량, 분산/병렬 환경에 대해서 주로 대화를 나눴는데 세 명 모두 각자의 작업을 목표한 만큼 거의 달성했다.
     4. 질문을 잘하는 법이 중요하다고 느낀다.
        1. A라는 상황에 어떻게 대처해야 하는가? 라는 질문은 그 상황을 얼마나 이해하고 있느냐에 따라 다른 답이 나오는 것 같다.
        2. 서로 같은 수준으로 이해하고 있는지, 그렇지 않다면 범위를 점점 좁혀가야 한다고 느꼈다.
        3. (은태님) 스트리밍한 데이터에 대한 장애 대응 중 재전송을 한 데이터와 새로 받아온 데이터를 어떻게 구분할 수 있을까?
           1. (주영) 일단 스트리밍한 환경에서 어떤 제약사항이 있는지 잘 알지 못하고, 예시에 대해서는 무시해도 좋을 중요도의 문제라고 느껴졌다.
            - 케바케가 많을 것 같다. 하지만 비슷한 문제가 많이 발생하면서 생기는 어떤 원칙이 있을 것 같다.
### Problem
  1. Load 작업에서 DB 관련 작업 및 SQL을 정리하거나 학습하지 못했는데 퇴근 이후에 좀 더 개선하고 싶다.
  2. 지금까지 PEP 8 스타일 가이드를 찬찬히 읽고 적용해보지 못했다.xw
### Try:
  1. 틀린 답변이거나, 불필요한 질문일까봐 걱정하는데 시간 쓰지 말고 그냥 하고 피드백을 자주 받을 것
     1. 우선 스스로 생각해보고 나름의 결론까지는 내야 한다.
  2. 내일은 코드 레벨에서 세세한 코드 리뷰를 진행해볼 것
     1. 지금까지 팀원들과 주로 아키텍처, 설계, 시나리오 별로 문제 상황을 어떻게 해결할 지 위주로 대화를 많이 나눴다.
     2. 파이썬으로 체계적인 코드를 작성해본 경험이 부족한 만큼 많은 것을 질문하고 배울 수 있도록 다양한 관점에서 미리 고민해봐야겠다.
